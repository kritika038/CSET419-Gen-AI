# -*- coding: utf-8 -*-
"""cifar10lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-WIqhErTp3aYcttOpWSvEfqU9tXHUQRB
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    transforms.ToTensor(),
])

train_dataset = datasets.CIFAR10(
    root="./data", train=True, download=True, transform=transform
)

train_loader = DataLoader(
    train_dataset, batch_size=128, shuffle=True
)

class Encoder(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 32, 4, 2, 1),   # 32×16×16
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, 2, 1),  # 64×8×8
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1), # 128×4×4
            nn.ReLU(),
        )
        self.fc_mu = nn.Linear(128 * 4 * 4, latent_dim)
        self.fc_logvar = nn.Linear(128 * 4 * 4, latent_dim)

    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)
        return mu, logvar

def reparameterize(mu, logvar):
    std = torch.exp(0.5 * logvar)
    eps = torch.randn_like(std)
    return mu + eps * std

class Decoder(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.fc = nn.Linear(latent_dim, 128 * 4 * 4)
        self.deconv = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # 8×8
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, 2, 1),   # 16×16
            nn.ReLU(),
            nn.ConvTranspose2d(32, 3, 4, 2, 1),    # 32×32
            nn.Sigmoid()
        )

    def forward(self, z):
        x = self.fc(z)
        x = x.view(-1, 128, 4, 4)
        return self.deconv(x)

class VAE(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)

    def forward(self, x):
        mu, logvar = self.encoder(x)
        z = reparameterize(mu, logvar)
        recon = self.decoder(z)
        return recon, mu, logvar, z

def vae_loss(recon_x, x, mu, logvar):
    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')
    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss, kl_div, recon_loss + kl_div

latent_dim = 2   # keep 2 for visualization
model = VAE(latent_dim).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

epochs = 20

for epoch in range(epochs):
    model.train()
    total_loss = 0

    for x, _ in train_loader:
        x = x.to(device)
        optimizer.zero_grad()

        recon, mu, logvar, _ = model(x)
        recon_loss, kl_loss, loss = vae_loss(recon, x, mu, logvar)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch [{epoch+1}/{epochs}] | Loss: {total_loss/len(train_loader.dataset):.4f}")

model.eval()
with torch.no_grad():
    x, _ = next(iter(train_loader))
    x = x.to(device)
    recon, _, _, _ = model(x)

def show_reconstructions(original, recon, n=8):
    original = original.cpu()
    recon = recon.cpu()

    plt.figure(figsize=(16, 4))
    for i in range(n):
        # Original
        plt.subplot(2, n, i + 1)
        plt.imshow(np.transpose(original[i], (1,2,0)))
        plt.axis("off")

        # Reconstructed
        plt.subplot(2, n, i + 1 + n)
        plt.imshow(np.transpose(recon[i], (1,2,0)))
        plt.axis("off")

    plt.show()

show_reconstructions(x, recon)

latents = []
labels = []

model.eval()
with torch.no_grad():
    for x, y in train_loader:
        x = x.to(device)
        mu, _ = model.encoder(x)
        latents.append(mu.cpu())
        labels.append(y)

latents = torch.cat(latents).numpy()
labels = torch.cat(labels).numpy()

plt.figure(figsize=(8,6))
scatter = plt.scatter(
    latents[:,0], latents[:,1],
    c=labels, cmap="tab10", s=5
)
plt.colorbar(scatter)
plt.xlabel("Latent Dimension 1")
plt.ylabel("Latent Dimension 2")
plt.title("CIFAR-10 Latent Space (VAE)")
plt.show()

def vae_loss(recon_x, x, mu, logvar, beta=1.0):
    recon_loss = nn.functional.mse_loss(
        recon_x, x, reduction='sum'
    )

    kl_div = -0.5 * torch.sum(
        1 + logvar - mu.pow(2) - logvar.exp()
    )

    total_loss = recon_loss + beta * kl_div
    return recon_loss, kl_div, total_loss

beta = 0.5   # try: 0.1, 0.5, 1, 2, 4

for epoch in range(epochs):
    model.train()
    total_loss = 0

    for x, _ in train_loader:
        x = x.to(device)
        optimizer.zero_grad()

        recon, mu, logvar, _ = model(x)
        recon_loss, kl_loss, loss = vae_loss(
            recon, x, mu, logvar, beta
        )

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(
        f"Epoch {epoch+1} | "
        f"Recon: {recon_loss.item():.0f} | "
        f"KL: {kl_loss.item():.0f} | "
        f"Total: {total_loss/len(train_loader.dataset):.4f}"
    )

def kl_annealing(epoch, max_beta=4.0, warmup_epochs=10):
    return min(max_beta, max_beta * epoch / warmup_epochs)

for epoch in range(epochs):
    model.train()
    beta = kl_annealing(epoch, max_beta=4.0, warmup_epochs=10)

    for x, _ in train_loader:
        x = x.to(device)
        optimizer.zero_grad()

        recon, mu, logvar, _ = model(x)
        recon_loss, kl_loss, loss = vae_loss(
            recon, x, mu, logvar, beta
        )

        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch+1} | β = {beta:.2f}")

model.eval()

all_mu = []
all_labels = []

with torch.no_grad():
    for x, y in train_loader:
        x = x.to(device)
        mu, _ = model.encoder(x)   # use mean for stable latent space
        all_mu.append(mu.cpu())
        all_labels.append(y)

all_mu = torch.cat(all_mu).numpy()
all_labels = torch.cat(all_labels).numpy()

plt.figure(figsize=(8, 6))

for cls in range(10):
    idx = all_labels == cls
    plt.scatter(
        all_mu[idx, 0],
        all_mu[idx, 1],
        s=6,
        alpha=0.6,
        label=str(cls)
    )

plt.legend(title="Class", markerscale=2)
plt.xlabel("Latent Dimension 1")
plt.ylabel("Latent Dimension 2")
plt.title("Class-wise CIFAR-10 Latent Space")
plt.grid(True)
plt.show()

num_classes = 10

class Encoder(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 32, 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.ReLU(),
        )
        self.fc_mu = nn.Linear(128*4*4 + num_classes, latent_dim)
        self.fc_logvar = nn.Linear(128*4*4 + num_classes, latent_dim)

    def forward(self, x, y):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        x = torch.cat([x, y], dim=1)
        return self.fc_mu(x), self.fc_logvar(x)

class Decoder(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.fc = nn.Linear(latent_dim + num_classes, 128*4*4)
        self.deconv = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 3, 4, 2, 1),
            nn.Sigmoid()
        )

    def forward(self, z, y):
        z = torch.cat([z, y], dim=1)
        x = self.fc(z)
        x = x.view(-1, 128, 4, 4)
        return self.deconv(x)

model.eval()

all_mu = []
all_labels = []

with torch.no_grad():
    for x, y in train_loader:
        x = x.to(device)
        mu, _ = model.encoder(x)   # use mean (μ), not sampled z
        all_mu.append(mu.cpu())
        all_labels.append(y)

all_mu = torch.cat(all_mu).numpy()
all_labels = torch.cat(all_labels).numpy()

pip install umap-learn

import umap

reducer = umap.UMAP(
    n_components=2,
    n_neighbors=15,
    min_dist=0.1,
    random_state=42
)

mu_umap = reducer.fit_transform(all_mu)

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))

for cls in range(10):
    idx = all_labels == cls
    plt.scatter(
        mu_umap[idx, 0],
        mu_umap[idx, 1],
        s=6,
        alpha=0.7,
        label=str(cls)
    )

plt.legend(title="Class", markerscale=2)
plt.xlabel("UMAP Dimension 1")
plt.ylabel("UMAP Dimension 2")
plt.title("CIFAR-10 Latent Space (β-VAE + KL Annealing + UMAP)")
plt.grid(True)
plt.show()

