# -*- coding: utf-8 -*-
"""VAE Mnist .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dRtURCUTeqk6NvJNgXSsr1tu12wA8S_T
"""

pip install tensorflow matplotlib

import tensorflow as tf
from tensorflow.keras import layers, Model
import numpy as np
import matplotlib.pyplot as plt
import os

# ----------------------------
# Configuration
# ----------------------------
latent_dim = 2
epochs = 10
batch_size = 128
os.makedirs("vae_results", exist_ok=True)

# ----------------------------
# Load MNIST
# ----------------------------
(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()

x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

# ----------------------------
# Sampling Layer
# ----------------------------
class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        epsilon = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

# ----------------------------
# Encoder
# ----------------------------
encoder_inputs = layers.Input(shape=(28, 28, 1))
x = layers.Flatten()(encoder_inputs)
x = layers.Dense(256, activation="relu")(x)

z_mean = layers.Dense(latent_dim)(x)
z_log_var = layers.Dense(latent_dim)(x)
z = Sampling()([z_mean, z_log_var])

encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name="encoder")

# ----------------------------
# Decoder
# ----------------------------
latent_inputs = layers.Input(shape=(latent_dim,))
x = layers.Dense(256, activation="relu")(latent_inputs)
x = layers.Dense(28 * 28, activation="sigmoid")(x)
decoder_outputs = layers.Reshape((28, 28, 1))(x)

decoder = Model(latent_inputs, decoder_outputs, name="decoder")

# ----------------------------
# VAE Model
# ----------------------------
class VAE(Model):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def train_step(self, data):
        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = self.encoder(data)
            reconstruction = self.decoder(z)

            recon_loss = tf.reduce_mean(
                tf.keras.losses.binary_crossentropy(data, reconstruction)
            )
            kl_loss = -0.5 * tf.reduce_mean(
                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
            )

            total_loss = recon_loss + kl_loss

        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))

        return {
            "loss": total_loss,
            "reconstruction_loss": recon_loss,
            "kl_loss": kl_loss
        }

# ----------------------------
# Train
# ----------------------------
vae = VAE(encoder, decoder)
vae.compile(optimizer=tf.keras.optimizers.Adam())

vae.fit(
    x_train,
    epochs=epochs,
    batch_size=batch_size
)

# ----------------------------
# Generate Images
# ----------------------------
random_latent = np.random.normal(size=(16, latent_dim))
generated = decoder.predict(random_latent)

plt.figure(figsize=(4, 4))
for i in range(16):
    plt.subplot(4, 4, i + 1)
    plt.imshow(generated[i].reshape(28, 28), cmap="gray")
    plt.axis("off")

plt.savefig("vae_results/generated.png")
plt.show()

# ----------------------------
# Reconstruction Visualization
# ----------------------------
n = 10  # number of images
reconstructed = vae.decoder(
    vae.encoder(x_test[:n])[2]
)

plt.figure(figsize=(20, 4))

# Original images
for i in range(n):
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28), cmap="gray")
    plt.axis("off")
    if i == 0:
        ax.set_title("Original")

# Reconstructed images
for i in range(n):
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(reconstructed[i].numpy().reshape(28, 28), cmap="gray")
    plt.axis("off")
    if i == 0:
        ax.set_title("Reconstructed")

plt.show()

# ----------------------------
# Reconstruction & KL Loss
# ----------------------------
z_mean, z_log_var, z = encoder(x_test[:batch_size])
recon = decoder(z)

recon_loss = tf.reduce_mean(
    tf.keras.losses.binary_crossentropy(x_test[:batch_size], recon)
)

kl_loss = -0.5 * tf.reduce_mean(
    1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
)

print("Reconstruction Loss:", recon_loss.numpy())
print("KL Divergence Loss:", kl_loss.numpy())

latent_dim = 2

# ----------------------------
# Latent Space Visualization
# ----------------------------
z_mean, _, _ = encoder.predict(x_test, batch_size=128)

plt.figure(figsize=(8, 6))
plt.scatter(z_mean[:, 0], z_mean[:, 1], s=2, alpha=0.5)
plt.xlabel("z[0]")
plt.ylabel("z[1]")
plt.title("Latent Space Representation")
plt.show()

(_, y_test) = tf.keras.datasets.mnist.load_data()[1]

z_mean, _, _ = encoder.predict(x_test, batch_size=128)

plt.figure(figsize=(8, 6))
scatter = plt.scatter(
    z_mean[:, 0], z_mean[:, 1],
    c=y_test, cmap="tab10", s=3
)
plt.colorbar(scatter, label="Digit Label")
plt.xlabel("z[0]")
plt.ylabel("z[1]")
plt.title("Latent Space Colored by Digit Class")
plt.show()

total_loss = recon_loss + 0.1 * kl_loss

# ----------------------------
# KL Statistics Verification
# ----------------------------
z_mean, z_log_var, _ = encoder.predict(x_test[:1000], batch_size=128)

print("Mean of z_mean:", np.mean(z_mean, axis=0))
print("Std of z_mean:", np.std(z_mean, axis=0))
print("Mean log variance:", np.mean(z_log_var, axis=0))
print("Std log variance:", np.std(z_log_var, axis=0))

recon_history = []
kl_history = []

recon_history.append(recon_loss.numpy())
kl_history.append(kl_loss.numpy())

plt.figure(figsize=(8, 4))
plt.plot(recon_history, label="Reconstruction Loss")
plt.plot(kl_history, label="KL Divergence")
plt.xlabel("Training Steps")
plt.ylabel("Loss")
plt.legend()
plt.title("Reconstruction vs KL Divergence")
plt.show()

# ----------------------------
# Latent Traversal
# ----------------------------
grid_x = np.linspace(-2, 2, 10)
grid_y = np.linspace(-2, 2, 10)

plt.figure(figsize=(6, 6))

for i, yi in enumerate(grid_y):
    for j, xi in enumerate(grid_x):
        z = np.array([[xi, yi]])
        x_decoded = decoder.predict(z)
        digit = x_decoded[0].reshape(28, 28)

        ax = plt.subplot(10, 10, i * 10 + j + 1)
        plt.imshow(digit, cmap="gray")
        plt.axis("off")

plt.suptitle("Latent Space Traversal")
plt.show()

# ----------------------------
# Reconstruction Error Histogram
# ----------------------------
recon = decoder(encoder(x_test)[2])
recon_error = tf.reduce_mean(
    tf.keras.losses.binary_crossentropy(x_test, recon),
    axis=(1, 2)
)

plt.hist(recon_error.numpy(), bins=50)
plt.xlabel("Reconstruction Error")
plt.ylabel("Frequency")
plt.title("Reconstruction Error Distribution")
plt.show()

latent_dim = 8        # instead of 2
beta = 0.01           # weaker KL
epochs = 50

import tensorflow as tf
from tensorflow.keras import layers, Model
import numpy as np
import matplotlib.pyplot as plt

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

x_train = x_train.astype("float32") / 255.0
x_test  = x_test.astype("float32") / 255.0

x_train = np.expand_dims(x_train, -1)
x_test  = np.expand_dims(x_test, -1)

class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        epsilon = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

latent_dim = 2   # keep 2 for visualization

encoder_inputs = layers.Input(shape=(28, 28, 1))
x = layers.Conv2D(32, 3, activation="relu", strides=2, padding="same")(encoder_inputs)
x = layers.Conv2D(64, 3, activation="relu", strides=2, padding="same")(x)
x = layers.Flatten()(x)
x = layers.Dense(128, activation="relu")(x)

z_mean = layers.Dense(latent_dim, name="z_mean")(x)
z_log_var = layers.Dense(latent_dim, name="z_log_var")(x)
z = Sampling()([z_mean, z_log_var])

encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name="encoder")
encoder.summary()

latent_inputs = layers.Input(shape=(latent_dim,))
x = layers.Dense(7 * 7 * 64, activation="relu")(latent_inputs)
x = layers.Reshape((7, 7, 64))(x)
x = layers.Conv2DTranspose(64, 3, activation="relu", strides=2, padding="same")(x)
x = layers.Conv2DTranspose(32, 3, activation="relu", strides=2, padding="same")(x)
decoder_outputs = layers.Conv2DTranspose(1, 3, activation="sigmoid", padding="same")(x)

decoder = Model(latent_inputs, decoder_outputs, name="decoder")
decoder.summary()

class VAE(Model):
    def __init__(self, encoder, decoder, beta=0.01):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.beta = beta

    def train_step(self, data):
        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = self.encoder(data)
            reconstruction = self.decoder(z)

            recon_loss = tf.reduce_mean(
                tf.reduce_sum(
                    tf.keras.losses.binary_crossentropy(data, reconstruction),
                    axis=(1, 2)
                )
            )

            kl_loss = -0.5 * tf.reduce_mean(
                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
            )

            total_loss = recon_loss + self.beta * kl_loss

        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))

        return {
            "loss": total_loss,
            "reconstruction_loss": recon_loss,
            "kl_loss": kl_loss
        }

vae = VAE(encoder, decoder, beta=0.01)
vae.compile(optimizer=tf.keras.optimizers.Adam())

history = vae.fit(
    x_train,
    epochs=40,
    batch_size=128
)

plt.figure(figsize=(6,4))
plt.plot(history.history["reconstruction_loss"], label="Reconstruction Loss")
plt.plot(history.history["kl_loss"], label="KL Divergence")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Reconstruction vs KL Divergence")
plt.legend()
plt.show()

z_mean, _, _ = encoder.predict(x_test, batch_size=128)

plt.figure(figsize=(8,6))
scatter = plt.scatter(
    z_mean[:, 0],
    z_mean[:, 1],
    c=y_test,
    cmap="tab10",
    s=4,
    alpha=0.8
)
plt.colorbar(scatter, label="Digit Class")
plt.xlabel("z[0]")
plt.ylabel("z[1]")
plt.title("Latent Space Colored by Digit Class (CNN β-VAE)")
plt.show()

n = 10
z_mean, _, z = encoder.predict(x_test[:n])
recon = decoder.predict(z)

plt.figure(figsize=(20, 4))
for i in range(n):
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28), cmap="gray")
    plt.axis("off")
    if i == 0: ax.set_title("Original")

    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(recon[i].reshape(28, 28), cmap="gray")
    plt.axis("off")
    if i == 0: ax.set_title("Reconstructed")

plt.show()

class VAE(Model):
    def __init__(self, encoder, decoder, beta):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.beta = beta

    def train_step(self, data):
        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = self.encoder(data)
            reconstruction = self.decoder(z)

            recon_loss = tf.reduce_mean(
                tf.reduce_sum(
                    tf.keras.losses.binary_crossentropy(data, reconstruction),
                    axis=(1, 2)
                )
            )

            kl_loss = -0.5 * tf.reduce_mean(
                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
            )

            total_loss = recon_loss + self.beta * kl_loss

        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))

        return {
            "loss": total_loss,
            "reconstruction_loss": recon_loss,
            "kl_loss": kl_loss
        }

betas = [0.01, 0.1, 1.0, 5.0]
histories = {}

for beta in betas:
    print(f"\nTraining with beta = {beta}")

    vae = VAE(encoder, decoder, beta=beta)
    vae.compile(optimizer=tf.keras.optimizers.Adam())

    history = vae.fit(
        x_train,
        epochs=30,
        batch_size=128,
        verbose=0
    )

    histories[beta] = history

plt.figure(figsize=(8, 5))

for beta, history in histories.items():
    plt.plot(
        history.history["kl_loss"],
        label=f"KL (beta={beta})"
    )

plt.xlabel("Epochs")
plt.ylabel("KL Divergence")
plt.title("Effect of Beta on KL Divergence")
plt.legend()
plt.show()

plt.figure(figsize=(8, 5))

for beta, history in histories.items():
    plt.plot(
        history.history["reconstruction_loss"],
        label=f"Recon (beta={beta})"
    )

plt.xlabel("Epochs")
plt.ylabel("Reconstruction Loss")
plt.title("Effect of Beta on Reconstruction Loss")
plt.legend()
plt.show()

class VAE(Model):
    def __init__(self, encoder, decoder, beta_start=0.0):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.beta = tf.Variable(beta_start, trainable=False)

def train_step(self, data):
    # KL annealing
    self.beta.assign(tf.minimum(1.0, self.beta + 0.001))

    with tf.GradientTape() as tape:
        z_mean, z_log_var, z = self.encoder(data)
        reconstruction = self.decoder(z)

        recon_loss = tf.reduce_mean(
            tf.reduce_sum(
                tf.keras.losses.binary_crossentropy(data, reconstruction),
                axis=(1, 2)
            )
        )

        kl_loss = -0.5 * tf.reduce_mean(
            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
        )

        total_loss = recon_loss + self.beta * kl_loss

    grads = tape.gradient(total_loss, self.trainable_weights)
    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))

    return {
        "loss": total_loss,
        "reconstruction_loss": recon_loss,
        "kl_loss": kl_loss,
        "beta": self.beta
    }

class VAE(tf.keras.Model):
    def __init__(self, encoder, decoder, beta_start=0.0):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.beta = tf.Variable(beta_start, trainable=False)

    # REQUIRED by Keras
    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        reconstruction = self.decoder(z)
        return reconstruction

    def train_step(self, data):
        # KL annealing
        self.beta.assign(tf.minimum(1.0, self.beta + 0.001))

        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = self.encoder(data)
            reconstruction = self.decoder(z)

            # Reconstruction loss
            recon_loss = tf.reduce_mean(
                tf.reduce_sum(
                    tf.keras.losses.binary_crossentropy(data, reconstruction),
                    axis=(1, 2)
                )
            )

            # KL divergence
            kl_loss = -0.5 * tf.reduce_mean(
                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
            )

            total_loss = recon_loss + self.beta * kl_loss

        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))

        return {
            "loss": total_loss,
            "reconstruction_loss": recon_loss,
            "kl_loss": kl_loss,
            "beta": self.beta
        }

vae = VAE(encoder, decoder, beta_start=0.0)
vae.compile(optimizer=tf.keras.optimizers.Adam())

history = vae.fit(
    x_train,
    epochs=40,
    batch_size=128
)

print("Final beta value:", vae.beta.numpy())

plt.figure(figsize=(6,4))
plt.plot(history.history["reconstruction_loss"], label="Reconstruction Loss")
plt.plot(history.history["kl_loss"], label="KL Divergence")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Reconstruction vs KL Divergence (KL Annealing)")
plt.legend()
plt.show()

z_mean, _, _ = encoder.predict(x_test, batch_size=128)

plt.figure(figsize=(8,6))
scatter = plt.scatter(
    z_mean[:, 0],
    z_mean[:, 1],
    c=y_test,
    cmap="tab10",
    s=4,
    alpha=0.8
)
plt.colorbar(scatter, label="Digit Class")
plt.xlabel("z[0]")
plt.ylabel("z[1]")
plt.title("Latent Space after KL Annealing (CNN β-VAE)")
plt.show()

n = 10
z_mean, _, z = encoder.predict(x_test[:n])
recon = decoder.predict(z)

plt.figure(figsize=(20, 4))
for i in range(n):
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28), cmap="gray")
    plt.axis("off")
    if i == 0: ax.set_title("Original")

    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(recon[i].reshape(28, 28), cmap="gray")
    plt.axis("off")
    if i == 0: ax.set_title("Reconstructed")

plt.show()

